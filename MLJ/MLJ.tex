% Mo Jabeen Template for docs 

\documentclass[11pt]{scrartcl} % Font size

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------


\title{	
	\normalfont\normalsize
	\vspace{20pt} % Whitespace
	{\huge MLJ : Notes}\\ % The meh
	\vspace{12pt} % Whitespace
	\rule{\linewidth}{2pt}\\ % Thick bottom horizontal rule
}

\author{\small Dainish Jabeen} % Your name

\date{\normalsize\today} % Today's date (\today) or a custom date

\begin{document}

\maketitle % Print the title

\section{General}

MLJ is a machine learning toolbox for Julia, that wraps a large number of models and provides
great tools such as resampling and evaluation \cite{Blaom2020} \cite{blaom2020flexible}.\\

Models are structs storing hyperparameters.

\begin{lstlisting}[
	caption= Basics, % Caption above the listing
	language=python, % Use Julia functions/syntax highlighting
	frame=single, % Frame around the code listing
	showstringspaces=false, % Don't put marks in string spaces
	numbers=left, % Line numbers on left
	numberstyle=\large, % Line numbers styling
	]

	#Split data randomly with seed (rng) 
	#	if y and X need to be separated
	y, X = unpack(df,==(:col);rng=123); 

	df |> pretty #Output pretty version

	#Load model
	VAR = @load model pkg=""

	var = VAR() #Default parameters

	#evaluate model with cross validation
	 via error measures

	evaluate(var,X,y,resampling=CV(shuffle=true),measures=[rms])
\end{lstlisting}

\subsection{Types}

Each model has an expected variable type needed to train it:\\

\textbf{target\_scitype(model)} provides type needed.\\

\begin{lstlisting}[
	caption= Change type, % Caption above the listing
	language=python, % Use Julia functions/syntax highlighting
	frame=single, % Frame around the code listing
	showstringspaces=false, % Don't put marks in string spaces
	numbers=left, % Line numbers on left
	numberstyle=\large, % Line numbers styling
	]

	#Change type of var

	y = coerce(df, :col=type,...)

\end{lstlisting}

\newpage

\section{Machines}

Used to store training outcomes.

\begin{lstlisting}[
	caption= Machines , % Caption above the listing
	language=python, % Use Julia functions/syntax highlighting
	frame=single, % Frame around the code listing
	showstringspaces=false, % Don't put marks in string spaces
	numbers=left, % Line numbers on left
	numberstyle=\large, % Line numbers styling
	]

	mach = machine(model,X,y)
	
	#70:30 partition giving an index vector
	train, test = partition(eachindex(y),0.7)

	fit!(mach,train)
	yhat = predict(mach,X[text,:]);

	#error rate
	misclassification_rate(yhat, test)

\end{lstlisting}

Can also evaulate! machines.\\

Machines will throw a warning if there is a mixmatch of types (continous and count).

\subsection{Prediction}

To predict with a given probability for a class use \textbf{broadcast}.\\

For a matrix of all classes: \textbf{pdf}.\\

For an output of a class: \textbf{predict\_mode}.

\subsection{Inspection}

Two methods: fitted\_params and report.\\

\textbf{Fitted params:} The learned parameters for a machine.\\

\textbf{report:} More detailed stats on machine.

\section{Hyperparameter tuning}

Check notebook Resampling(Julia-MLJ) for example.

\begin{itemize}
	\item Create ensemble model
	\item Create ranges for parameters
	\item wrap in tuned model
	\item create mach, fit data
	\item report on best model
	\item mach will become the best model
\end{itemize}

\textbf{learning\_curve()} gives a performance line for a tuning parameter. 

\section{Pipeline}

Linear set of models chained together, that can be evaluated and used as a single model.\\

For example: Change var type |> tune parameter |> model |> train.\\

Similar to recipes, pipelines are a series of transforms on data, then fed into a model.

\begin{lstlisting}[
	caption= Pipeline example, % Caption above the listing
	language=python, % Use Julia functions/syntax highlighting
	frame=single, % Frame around the code listing
	showstringspaces=false, % Don't put marks in string spaces
	numbers=left, % Line numbers on left
	numberstyle=\large, % Line numbers styling
	]

	pipe = X |> Standardizer |> Model

	TunedModel(model=pipe)
	Machine(pipe,X,y)
	evaulate(pipe)

\end{lstlisting}

\section{Resampling}

\begin{lstlisting}[
	caption= Resampling, % Caption above the listing
	language=python, % Use Julia functions/syntax highlighting
	frame=single, % Frame around the code listing
	showstringspaces=false, % Don't put marks in string spaces
	numbers=left, % Line numbers on left
	numberstyle=\large, % Line numbers styling
	]

	#Split data into sections of chosen percentage, 
	#	can also shuffle
	holdout = Holdout(; fraction_train=0.7,
                     shuffle=nothing,
                     rng=nothing)

	# Cross validate over number of folds
	cv = CV(; nfolds=6,  shuffle=nothing, rng=nothing)

	# For classification problems aims to retain the connection
	#	between the predictors in train and test with the 
	#	response class level.

	stratified_cv = StratifiedCV(; nfolds=6,
                               shuffle=false,
                               rng=Random.GLOBAL_RNG)


	#CV for when observations are chronological and not expected 
	#	to be independent.

	tscv = TimeSeriesCV(; nfolds=4)


\end{lstlisting}

\section{Parallel}

Using the parameter \textbf{acceleration} in train!, can be set to:

\begin{itemize}
	\item CPU1 : Single threaded
	\item CPUProcesses: Multi process (core)
	\item CPUThreads: Mutli threaded
	\item CUDALibs: GPU Computation
\end{itemize}

Or can set \textbf{MLJ.default\_resource()} to one of the above.

\section{Transformers}

Work similar to models, but only require the input data as they are unsupervised.

\begin{lstlisting}[
	caption= Standardizer, % Caption above the listing
	language=python, % Use Julia functions/syntax highlighting
	frame=single, % Frame around the code listing
	showstringspaces=false, % Don't put marks in string spaces
	numbers=left, % Line numbers on left
	numberstyle=\large, % Line numbers styling
	]

	#features are the chosen predictors to standardise.
	model = Standardizer(features=)

\end{lstlisting}

\subsection{PCA}

\begin{itemize}
	\item PCA: Linear
	\item Kernal PCA: Non linear
	\item Probabilistic: Gaussian 
\end{itemize}

\begin{lstlisting}[
	caption= Standardizer, % Caption above the listing
	language=python, % Use Julia functions/syntax highlighting
	frame=single, % Frame around the code listing
	showstringspaces=false, % Don't put marks in string spaces
	numbers=left, % Line numbers on left
	numberstyle=\large, % Line numbers styling
	]

	#params include max output dims,kernel,solver,inverse transform
	#,convergence total, max iterations.
	model = KernelPCA()

\end{lstlisting}


\section{Performance}

\[ConfusionMatrix()(y_{pred},y)\]

\[ false\;pos\;rate,\; true\; postive\; rate,\; thresholds = roc(y_{probpredict},y) \]

\section{Save}

\begin{lstlisting}[
	caption= Saving machine, % Caption above the listing
	language=python, % Use Julia functions/syntax highlighting
	frame=single, % Frame around the code listing
	showstringspaces=false, % Don't put marks in string spaces
	numbers=left, % Line numbers on left
	numberstyle=\large, % Line numbers styling
	]

	# SAVE
	MLJ.save("name.jls",mach)

	# LOAD
	mach = machine("name.jlso")

\end{lstlisting}


\newpage

\section{Scikit Learn: Notes}

Many of the models used in MLJ are derived from Scikit learn \cite{scikit-learn}.

\subsection{Support vector machine}

\textbf{Advantages:}
\begin{itemize}
	\item Works well with high dimensional spaces
	\item Works well if p > n
	\item memory efficient
	\item Versatile (different kernels)
\end{itemize}

Three types:\\
\textbf{SVC}: Radial kernel, with cost as key balancing hyperparameter.\\
\textbf{NuSVC}: Radial kernel, with number of support vectors as key balancing hyperparameter.\\
\textbf{LinearSVC}: Linear kernel.\\

OneclassSVM is used for outlier detection.\\

\begin{equation}
	O(n_{factor}*n^3_{samples})
\end{equation}

For regression SVR, NuSVR and LinearSVR exist.\\

If certain classes or samples have higher importance weights can be used.

\subsubsection{MultiClass}

If more than two classes are used SVC and NuSVC use one versus one model approach, linear uses
one versus all.

\begin{equation}
	O(1-1) = \frac{n(n-1)}{2}
\end{equation}

\begin{equation}
	O(1-n) = n
\end{equation}

\subsubsection{Probabilistic value}

Use model ProbabilisticSVC, to provide probability values.\\

Use wrapper BinaryThresholdPredictor(model; threshold=0.5) to change to deterministic.

\subsubsection{Parameters}

\textbf{Cost:} Trade off between misclassification and decision surface simplicity, lower C
gives a simpler decision surface.\\

\textbf{Gamma:} How much influence an observation has.

\subsubsection{Tips}

\begin{itemize}
	\item Data should be scaled
	\item L1 penalization yields a sparse selection of predictors.
	\item C and gamma are key parameters
	\item Use Grid search CV
\end{itemize}

\subsection{Outlier Detection}

\textbf{Outlier:} Data far from others, in the training data.\\
\textbf{Novelty:} Outlier not in training data.\\

Unsupervised methods include using estimator.fit and estimator.predict functions. Outlier methods
isolation forest and neighbor local outlier factor. OneClassSVM needs training for effective 
outlier detection.\\

Primary novelty detection is one OneClassSVM.

\newpage

\printbibliography

%----------------------------------------------------------------------------------------
%	FIGURE EXAMPLE
%----------------------------------------------------------------------------------------

% \begin{figure}[h] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
% 	\centering
% 	\includegraphics[width=0.5\columnwidth]{IMAGE_NAME.jpg} % Example image
% 	\caption{European swallow.}
% \end{figure}

%----------------------------------------------------------------------------------------
% MATH EXAMPLES
%----------------------------------------------------------------------------------------

% \begin{align} 
% 	\label{eq:bayes}
% 	\begin{split}
% 		P(A|B) = \frac{P(B|A)P(A)}{P(B)}
% 	\end{split}					
% \end{align}

%----------------------------------------------------------------------------------------
%	LIST EXAMPLES
%----------------------------------------------------------------------------------------

% \begin{itemize}
% 	\item First item in a list 
% 		\begin{itemize}
% 		\item First item in a list 
% 			\begin{itemize}
% 			\item First item in a list 
% 			\item Second item in a list 
% 			\end{itemize}
% 		\item Second item in a list 
% 		\end{itemize}
% 	\item Second item in a list 
% \end{itemize}

%------------------------------------------------

% \subsection{Numbered List}

% \begin{enumerate}
% 	\item First item in a list 
% 	\item Second item in a list 
% 	\item Third item in a list
% \end{enumerate}

%----------------------------------------------------------------------------------------
%	TABLE EXAMPLE
%----------------------------------------------------------------------------------------

% \section{Interpreting a Table}

% \begin{table}[h] % [h] forces the table to be output where it is defined in the code (it suppresses floating)
% 	\centering % Centre the table
% 	\begin{tabular}{l l l}
% 		\toprule
% 		\textit{Per 50g} & \textbf{Pork} & \textbf{Soy} \\
% 		\midrule
% 		Energy & 760kJ & 538kJ\\
% 		Protein & 7.0g & 9.3g\\
% 		\bottomrule
% 	\end{tabular}
% 	\caption{Sausage nutrition.}
% \end{table}

%----------------------------------------------------------------------------------------
%	CODE LISTING EXAMPLE
%----------------------------------------------------------------------------------------

% \begin{lstlisting}[
% 	caption= Macro definition, % Caption above the listing
% 	language=python, % Use Julia functions/syntax highlighting
% 	frame=single, % Frame around the code listing
% 	showstringspaces=false, % Don't put marks in string spaces
% 	numbers=left, % Line numbers on left
% 	numberstyle=\large, % Line numbers styling
% 	]

% 	CODE

% \end{lstlisting}

%----------------------------------------------------------------------------------------
%	CODE LISTING FILE EXAMPLE
%----------------------------------------------------------------------------------------

% \lstinputlisting[
% 	caption=Luftballons Perl Script., % Caption above the listing
% 	label=lst:luftballons, % Label for referencing this listing
% 	language=Perl, % Use Perl functions/syntax highlighting
% 	frame=single, % Frame around the code listing
% 	showstringspaces=false, % Don't put marks in string spaces
% 	numbers=left, % Line numbers on left
% 	numberstyle=\tiny, % Line numbers styling
% 	]{luftballons.pl}

%----------------------------------------------------------------------------------------
%	BIB EXAMPLE
%----------------------------------------------------------------------------------------

% Using \texttt{biblatex} you can display a bibliography divided into sections, depending on citation type. 
% Let's cite! Einstein's journal paper \cite{einstein} and Dirac's book \cite{dirac} are physics-related items. 
% Next, \textit{The \LaTeX\ Companion} book \cite{latexcompanion}, Donald Knuth's website \cite{knuthwebsite}, \textit{The Comprehensive Tex Archive Network} (CTAN) \cite{ctan} are \LaTeX-related items; but the others, Donald Knuth's items, \cite{knuth-fa,knuth-acp} are dedicated to programming. 

% \medskip

% \printbibliography[
% heading=bibintoc,
% title={Whole bibliography}
% ] %Prints the entire bibliography with the title "Whole bibliography"

% %Filters bibliography
% \printbibliography[heading=subbibintoc,type=article,title={Articles only}]
% \printbibliography[type=book,title={Books only}]

% \printbibliography[keyword={physics},title={Physics-related only}]
% \printbibliography[keyword={latex},title={\LaTeX-related only}]

\end{document}